#!/usr/local/bin/python
#coding=utf-8

# æ­¤è„šæœ¬ç”¨äºæ›´æ–°æœ¬ç«å“å…¬ä¼—å·æœ€æ–°å‘å¸ƒæ–‡ç« çš„æ—¥æœŸ

# éœ€è¦ pip å®‰è£…çš„å¤–éƒ¨åº“
import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
# lxml éœ€è¦å®‰è£…ä½†ä¸ç”¨å¼•å…¥

# å†…ç½®åº“
import json
import time
import datetime
import re
from bson.objectid import ObjectId

# æ¥å£é…ç½®
url_templ = 'https://weixin.sogou.com/weixin?type=1&query={}'
headers1 = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
    'Cookie': 'ABTEST=2|1549803206|v1; IPLOC=CN4509; SUID=981289B4771A910A000000005C601EC6; SUID=981289B41620940A000000005C601EC7; weixinIndexVisited=1; SUV=005F1DC2B48912985C60204C3DD7F127; pgv_pvi=5303055360; JSESSIONID=aaahygNN6x7JTa3qTM7Hw; pgv_si=s2559925248; ld=zyllllllll2tYyoxlllllVeOVDklllllHw6vulllllolllllRklll5@@@@@@@@@@; LSTMV=560%2C156; LCLKINT=1506; sct=68; PHPSESSID=nlds2d4qdqj613pa7c4rfb6ru4; SNUID=B01A2E29F7F27620FD93157BF79FBFD8; seccodeRight=success; successCount=1|Thu, 14 Feb 2019 09:16:16 GMT',
    'Host': 'weixin.sogou.com'}

headers2 = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
    'Host': 'mp.weixin.qq.com'}

# æ•°æ®åº“é…ç½®
client = MongoClient()
db = client.newrank
account_collection = db.sharecar_weixin_account

# å…¶ä»–é…ç½®
today = str(datetime.date.today())

def get_url(id):
    url = url_templ.format(id)
    r = requests.get(url, headers = headers1)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'lxml')
        els = soup.select('p.tit a')
        if els:
            return els[0]['href']
        else:
            print('\n' + id)
            return None
    else:
        print('%-40s' % ('ğŸ’©  è¯·æ±‚å¤±è´¥ï¼š{}'.format(url)))
        return None

def get_publish_date(url):
    r = requests.get(url, headers = headers2)
    if r.status_code == 200:
        m = re.search(r'.*msgList\s\=\s(\{.*\});\n\s+sea', r.text)
        if m == None:
            return None
        else:
            msg_list = json.loads(m.group(1))['list']
            if msg_list:
                return time.strftime('%Y-%m-%d',time.localtime(msg_list[0]['comm_msg_info']['datetime']))
            else:
                return None
    else:
        return None

# ä½¿ç”¨å…¬ä¼—å· id æœç´¢ï¼Œå¾—å‡ºä¸»é¡µé“¾æ¥ï¼Œåœ¨ä¸»é¡µè·å–æœ€æ–°å‘å¸ƒæ–‡ç« çš„æ—¥æœŸ
# æœç‹—å¾®ä¿¡æ— æ³•æœç´¢åˆ°æœåŠ¡å·ï¼Œéœ€è¦æ‰‹åŠ¨æ›´æ–°å‘å¸ƒä¿¡æ¯
accounts = account_collection.find({
        'is_relevant': True,
        'is_valid': True,
        'updated_at': {'$exists': False},
        'type': 1,
    },{'id': 1, 'name': 1})
for account in accounts:
    print('%-40s' % ('âŒ›ï¸  é‡‡é›†ä¸­ï¼š{}ï¼ˆ{}ï¼‰...'.format(account['name'], account['id'])), end='\r')
    url = get_url(account['id'])
    time.sleep(15)

    if url:
        publish_date = get_publish_date(url)
        
        if publish_date:
            print('%-50s' % ('ğŸ‘‰  {}ï¼ˆ{}ï¼‰{}'.format(account['name'], account['id'], publish_date)), end='\r')
            account_collection.update({
                        '_id': ObjectId(account['_id'])}, {
                            '$set': {
                                'latest_published_at': publish_date,
                                'updated_at': today }})
        else:
            print('%-50s' % ('ğŸ’©  è¯¦æƒ…è¯·æ±‚å¤±è´¥ï¼š{}'.format(url)))
            client.close()
    else:
        print('%-50s' % ('ğŸ’©  æœç´¢è¯·æ±‚å¤±è´¥ï¼š{}'.format(url)))

    time.sleep(15)

client.close()